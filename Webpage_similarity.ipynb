{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcda7ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用网页相似度对比小程序！本程序需运行Google Word2Vec模型,\n",
      "您本地若不存在该模型，则系统将自动为您下载，请保证网络通畅！\n",
      "模型下载完毕后，请解压该模型包并将模型文件放入Jupyter根目录并重新运行本程序！\n",
      "模型已检测到！请耐心等待模型加载...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-05-22T10:21:02.339894', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载完毕！\n",
      "请输入第一个网址:\n",
      "www.google.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error fetching CSS from /xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d: Invalid URL '/xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d': No scheme supplied. Perhaps you meant http:///xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入第二个网址:\n",
      "www.baidu.com\n",
      "两个网页的相似度为: 0.769131064414978\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging  \n",
    "import requests  \n",
    "import numpy as np\n",
    "import cssutils\n",
    "import zipfile\n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup, Comment  \n",
    "from urllib.parse import urljoin  \n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    " \n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)  \n",
    "logger = logging.getLogger(__name__)  \n",
    "  \n",
    "def clean_html(html_content):  \n",
    "    #Parsing HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  \n",
    "      \n",
    "    # Remove excess space and line breaks\n",
    "    for tag in soup.findAll(text=True):  \n",
    "        if tag.strip().replace(\"\\n\", \"\") == \"\" and tag.parent.name not in ['pre', 'textarea']:  \n",
    "            tag.extract()  \n",
    "      \n",
    "    # Remove comments\n",
    "    comments = soup.findAll(text=lambda text: isinstance(text, Comment))  \n",
    "    [comment.extract() for comment in comments]  \n",
    "      \n",
    "    # Extract and return cleaned HTML\n",
    "    cleaned_html = str(soup).replace(\">\\n<\", \"><\")  # Remove excess spaces between labels \n",
    "    cleaned_html = re.sub(r\">\\s+<\", \"><\", cleaned_html)  # Remove excess spaces between labels\n",
    "    cleaned_html = re.sub(r'\\s+', ' ', cleaned_html)  # Replace all consecutive white space characters with one space   \n",
    "    cleaned_html = cleaned_html.replace(' {', '{').replace(' }', '}')  # Remove spaces inside and outside of curly braces\n",
    "    cleaned_html = cleaned_html.replace(': ', ':').replace('; ', ';')  # Remove spaces between attributes and values, as well as spaces after semicolons \n",
    "    cleaned_html = cleaned_html.strip()  # Remove space characters from both ends of the string\n",
    "    \n",
    "    return cleaned_html  \n",
    "\n",
    "def clean_css(css_content):\n",
    "    css_text = css_content\n",
    "    cleaned_css = re.sub(r'>\\s+<', '><', css_text)  # Remove excess spaces between labels\n",
    "    cleaned_css = re.sub(r'\\s+', ' ', cleaned_css)  # Replace all consecutive white space characters with one space  \n",
    "    cleaned_css = cleaned_css.replace(' {', '{').replace(' }', '}')  # Remove spaces inside and outside of curly braces\n",
    "    cleaned_css = cleaned_css.replace(': ', ':').replace('; ', ';')  # Remove spaces between attributes and values, as well as spaces after semicolons \n",
    "    cleaned_css = cleaned_css.strip()  # Remove space characters from both ends of the string\n",
    "    return cleaned_css \n",
    "  \n",
    "def fetch_and_clean_webpage(response):  \n",
    "    try:          \n",
    "        # Parsing HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "          \n",
    "        # Extract and clean internal CSS styles (if any)\n",
    "        internal_css = ''  \n",
    "        for style_tag in soup.find_all('style'):  \n",
    "            internal_css += style_tag.get_text()  \n",
    "        cleaned_internal_css = clean_css(internal_css)  \n",
    "          \n",
    "        # Separate HTML and CSS (external style sheets) \n",
    "        css_links = soup.find_all('link', {'rel': 'stylesheet'})  \n",
    "        css_contents = \"\"  \n",
    "          \n",
    "        # Get and clean up all external CSS file contents\n",
    "        for css_link in css_links:  \n",
    "            css_url = css_link.get('href')  \n",
    "            if css_url:  \n",
    "                full_css_url = urljoin(url, css_url)  # Build a complete CSS URL using urljoin  \n",
    "                try:  \n",
    "                    css_response = requests.get(full_css_url, timeout=100)  \n",
    "                    css_response.raise_for_status()  \n",
    "                    css_content = css_response.text  \n",
    "                    cleaned_css = clean_css(css_content)  \n",
    "                    css_contents += cleaned_css  \n",
    "                except requests.RequestException as e:  \n",
    "                    logger.error(f\"Error fetching CSS from {full_css_url}: {e}\")  \n",
    "          \n",
    "        # Clean up and return HTML content and CSS content (including internal and external styles) \n",
    "        cleaned_html = clean_html(str(soup))  \n",
    "        return cleaned_html, css_contents + cleaned_internal_css  \n",
    "    except requests.RequestException as e:  \n",
    "        logger.error(f\"Error fetching webpage from {url}: {e}\")  \n",
    "        raise  # Reraise the exception after logging  \n",
    "    except Exception as e:  \n",
    "        logger.exception(f\"An unexpected error occurred while processing {url}: {e}\")  \n",
    "        raise  # Reraise the exception after logging  \n",
    "\n",
    "def main(url):\n",
    "    \n",
    "    print(\"Welcome to the webpage similarity comparison program! This program needs to run the Google Word2Vec model,\")\n",
    "    print(\"If the model does not exist locally, the system will automatically download it for you. Please ensure smooth network connection!\")\n",
    "    print(\"After downloading the model, please unzip the model package and place the model file in the Jupyter root directory and run this program again!\") \n",
    "    try:\n",
    "        # Load Google News的Word2Vec Model \n",
    "        print(\"Model detected! Please be patient while the model loads...\") \n",
    "        model_path = 'GoogleNews-vectors-negative300.bin'  \n",
    "        model = KeyedVectors.load_word2vec_format(model_path, binary=True)  \n",
    "    except:\n",
    "        wv = api.load('word2vec-google-news-300')\n",
    "        print(\"Please restart the program!\")\n",
    "        exit()\n",
    "    \n",
    "    url = ''\n",
    "    # Locate the contents of the destinated webpage\n",
    "    print(\"The model has been loaded completely！\") \n",
    "    print(\"Please enter the first website address:\")\n",
    "    url = 'http://' + input().strip()\n",
    "    response = requests.get(url, timeout=100)\n",
    "    try:  \n",
    "        cleaned_html, cleaned_css = fetch_and_clean_webpage(response)\n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    print(\"Please enter the second website address:\")\n",
    "    url = 'http://' + input().strip()  \n",
    "    response = requests.get(url, timeout=100)\n",
    "\n",
    "    try:  \n",
    "        cleaned_html_c, cleaned_css_c = fetch_and_clean_webpage(response)\n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    words1 = word_tokenize(cleaned_html) \n",
    "    words2 = word_tokenize(cleaned_html_c)     \n",
    "\n",
    "    # Convert vocabulary into vectors and calculate the average vector\n",
    "    vector1 = np.mean([model[word] for word in words1 if word in model], axis=0)  \n",
    "    vector2 = np.mean([model[word] for word in words2 if word in model], axis=0)  \n",
    "\n",
    "    # Calculate cosine similarity \n",
    "    similarity = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]  \n",
    "    print(f'The similarity between the two websites is: {similarity}')\n",
    "    \n",
    "if __name__== \"__main__\" :\n",
    "    url = ''\n",
    "    main(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ced2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
