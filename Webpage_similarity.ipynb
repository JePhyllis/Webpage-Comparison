{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcda7ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.keyedvectors:loading projection weights from GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欢迎使用网页相似度对比小程序！本程序需运行Google Word2Vec模型,\n",
      "您本地若不存在该模型，则系统将自动为您下载，请保证网络通畅！\n",
      "模型下载完毕后，请解压该模型包并将模型文件放入Jupyter根目录并重新运行本程序！\n",
      "模型已检测到！请耐心等待模型加载...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-05-22T10:21:02.339894', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载完毕！\n",
      "请输入第一个网址:\n",
      "www.google.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error fetching CSS from /xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d: Invalid URL '/xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d': No scheme supplied. Perhaps you meant http:///xjs/_/ss/k=xjs.hp.eg0cCuMGRO8.L.X.O/am=AQAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAgQAAAAAAACAAAAAAIgAACAI/d=1/ed=1/rs=ACT90oFsGfnLe8OJfaH_K4qEI_DqP_66nQ/m=sb_he,d?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入第二个网址:\n",
      "www.baidu.com\n",
      "两个网页的相似度为: 0.769131064414978\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging  \n",
    "import requests  \n",
    "import numpy as np\n",
    "import cssutils\n",
    "import zipfile\n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup, Comment  \n",
    "from urllib.parse import urljoin  \n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    " \n",
    "# 忽略所有警告\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "# 配置日志记录  \n",
    "logging.basicConfig(level=logging.INFO)  \n",
    "logger = logging.getLogger(__name__)  \n",
    "  \n",
    "def clean_html(html_content):  \n",
    "    # 使用BeautifulSoup解析HTML  \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')  \n",
    "      \n",
    "    # 移除多余的空白和换行  \n",
    "    for tag in soup.findAll(text=True):  \n",
    "        if tag.strip().replace(\"\\n\", \"\") == \"\" and tag.parent.name not in ['pre', 'textarea']:  \n",
    "            tag.extract()  \n",
    "      \n",
    "    # 移除注释  \n",
    "    comments = soup.findAll(text=lambda text: isinstance(text, Comment))  \n",
    "    [comment.extract() for comment in comments]  \n",
    "      \n",
    "    # 提取并返回清理后的HTML  \n",
    "    cleaned_html = str(soup).replace(\">\\n<\", \"><\")  # 移除标签间的换行  \n",
    "    cleaned_html = re.sub(r\">\\s+<\", \"><\", cleaned_html)  # 移除标签间多余的空白字符\n",
    "    cleaned_html = re.sub(r'\\s+', ' ', cleaned_html)  # 将所有连续的空白字符替换为一个空格  \n",
    "    cleaned_html = cleaned_html.replace(' {', '{').replace(' }', '}')  # 去除大括号内外的空格  \n",
    "    cleaned_html = cleaned_html.replace(': ', ':').replace('; ', ';')  # 去除属性和值之间以及分号后的空格  \n",
    "    cleaned_html = cleaned_html.strip()  # 去除字符串两端的空白字符\n",
    "    \n",
    "    return cleaned_html  \n",
    "\n",
    "def clean_css(css_content):\n",
    "    css_text = css_content\n",
    "    cleaned_css = re.sub(r'>\\s+<', '><', css_text)  # 去除标签之间的多余空格  \n",
    "    cleaned_css = re.sub(r'\\s+', ' ', cleaned_css)  # 将所有连续的空白字符替换为一个空格  \n",
    "    cleaned_css = cleaned_css.replace(' {', '{').replace(' }', '}')  # 去除大括号内外的空格  \n",
    "    cleaned_css = cleaned_css.replace(': ', ':').replace('; ', ';')  # 去除属性和值之间以及分号后的空格  \n",
    "    cleaned_css = cleaned_css.strip()  # 去除字符串两端的空白字符  \n",
    "    return cleaned_css \n",
    "  \n",
    "def fetch_and_clean_webpage(response):  \n",
    "    try:          \n",
    "        # 解析HTML内容  \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')  \n",
    "          \n",
    "        # 提取并清理内部CSS样式（如果有）  \n",
    "        internal_css = ''  \n",
    "        for style_tag in soup.find_all('style'):  \n",
    "            internal_css += style_tag.get_text()  \n",
    "        cleaned_internal_css = clean_css(internal_css)  \n",
    "          \n",
    "        # 分离HTML和CSS（外部样式表）  \n",
    "        css_links = soup.find_all('link', {'rel': 'stylesheet'})  \n",
    "        css_contents = \"\"  \n",
    "          \n",
    "        # 获取并清理所有外部CSS文件内容  \n",
    "        for css_link in css_links:  \n",
    "            css_url = css_link.get('href')  \n",
    "            if css_url:  \n",
    "                full_css_url = urljoin(url, css_url)  # 使用urljoin构建完整的CSS URL  \n",
    "                try:  \n",
    "                    css_response = requests.get(full_css_url, timeout=100)  \n",
    "                    css_response.raise_for_status()  \n",
    "                    css_content = css_response.text  \n",
    "                    cleaned_css = clean_css(css_content)  \n",
    "                    css_contents += cleaned_css  \n",
    "                except requests.RequestException as e:  \n",
    "                    logger.error(f\"Error fetching CSS from {full_css_url}: {e}\")  \n",
    "          \n",
    "        # 清理并返回HTML内容以及CSS内容（包括内部和外部样式）  \n",
    "        cleaned_html = clean_html(str(soup))  \n",
    "        return cleaned_html, css_contents + cleaned_internal_css  \n",
    "    except requests.RequestException as e:  \n",
    "        logger.error(f\"Error fetching webpage from {url}: {e}\")  \n",
    "        raise  # Reraise the exception after logging  \n",
    "    except Exception as e:  \n",
    "        logger.exception(f\"An unexpected error occurred while processing {url}: {e}\")  \n",
    "        raise  # Reraise the exception after logging  \n",
    "\n",
    "def main(url):\n",
    "    \n",
    "    # 若本地缺乏该模型，则需激活该行语句进行下载\n",
    "    print(\"欢迎使用网页相似度对比小程序！本程序需运行Google Word2Vec模型,\")\n",
    "    print(\"您本地若不存在该模型，则系统将自动为您下载，请保证网络通畅！\")\n",
    "    print(\"模型下载完毕后，请解压该模型包并将模型文件放入Jupyter根目录并重新运行本程序！\") \n",
    "    try:\n",
    "        # 加载Google News的Word2Vec模型 \n",
    "        print(\"模型已检测到！请耐心等待模型加载...\") \n",
    "        model_path = 'GoogleNews-vectors-negative300.bin'  \n",
    "        model = KeyedVectors.load_word2vec_format(model_path, binary=True)  \n",
    "    except:\n",
    "        wv = api.load('word2vec-google-news-300')\n",
    "        print(\"Please restart the program!\")\n",
    "        exit()\n",
    "    \n",
    "    url = ''\n",
    "    # 获取并清理指定网页的内容 \n",
    "    print(\"模型已加载完毕！\") \n",
    "    print(\"请输入第一个网址:\")\n",
    "    url = 'http://' + input().strip()\n",
    "    response = requests.get(url, timeout=100)\n",
    "    try:  \n",
    "        cleaned_html, cleaned_css = fetch_and_clean_webpage(response)\n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    print(\"请输入第二个网址:\")\n",
    "    url = 'http://' + input().strip()  \n",
    "    response = requests.get(url, timeout=100)\n",
    "\n",
    "    try:  \n",
    "        cleaned_html_c, cleaned_css_c = fetch_and_clean_webpage(response)\n",
    "    except Exception as e:  \n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    words1 = word_tokenize(cleaned_html) \n",
    "    words2 = word_tokenize(cleaned_html_c)     \n",
    "\n",
    "    # 将词汇转换为向量并计算平均向量  \n",
    "    vector1 = np.mean([model[word] for word in words1 if word in model], axis=0)  \n",
    "    vector2 = np.mean([model[word] for word in words2 if word in model], axis=0)  \n",
    "\n",
    "    # 计算余弦相似度  \n",
    "    similarity = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]  \n",
    "    print(f'两个网页的相似度为: {similarity}')\n",
    "    \n",
    "if __name__== \"__main__\" :\n",
    "    url = ''\n",
    "    main(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ced2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
